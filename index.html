<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Course Project
  | ECE, Virginia Tech | Fall 2021: ECE 4554/5554</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Lane Detection System For ADAS</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Krishna Kanth Vuppala Narasimha, Beenaa Motiram Salian, Sriamoghavarsha Beerangi Srinivasa</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2023 ECE 4554/5554 Computer Vision: Course Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Virginia Tech</span>
<hr>

<!-- Goal -->
<h3>Abstract</h3>

With the increasing demand for safer and more efficient autonomous driving, our project tackles the crucial task of enhancing lane detection systems. Accurate lane detection is pivotal for safe navigation and has far-reaching implications for the future of self-driving vehicles. We adopted a comprehensive approach by implementing and comparing two advanced deep learning models, LaneNet and SegNet, both trained on a diverse dataset of highway images. Our evaluation, conducted within the Carla Simulator, aimed to provide insights into the models' performance in real-world driving scenarios
<br><br>

<!-- figure -->
<!-- Main Illustrative Figure --> 
<h3>Teaser figure</h3>
<br><br>
<div style="text-align: center;">
<img style="height: 150px;" alt="Teaser Image" src="teaser_img.png">
</div>
<br><br>
<div style="text-align: center;">
<img style="height: 200px;" alt="Teaser Image" src="https://thegradient.pub/content/images/2018/05/semseg.gif">
</div>
<br><br>

<!-- Introduction -->
<h3>Introduction</h3>
The quest for achieving precision and reliability in lane detection has been a focal point in the world of autonomous vehicles and advanced driving assistance systems. Ensuring accurate lane detection is crucial for vehicle safety, facilitating tasks like lane-keeping and path planning, especially in intricate urban environments. Traditional computer vision methods, though pioneering, often falter when confronted with real-world challenges, such as faded lane markings, unpredictable shadows, or adverse weather conditions.
<br><br>
In the deep learning realm, models like LaneNet and SegNet have emerged as potential game-changers for lane detection. LaneNet, with its emphasis on semantic and instance segmentation, is adept at distinguishing overlapping lanes, adapting effectively to intricate road scenarios. In contrast, SegNet, drawing inspiration from the VGG16 architecture, is renowned for its semantic segmentation capabilities, offering pixel-wise classification that is crucial for clear lane boundary identification in varied contexts. Both models, tailored for regular RGB photographs, address many of the limitations intrinsic to conventional methods.
<br><br>
In this project, rather than merging their capabilities, we endeavor to conduct a comprehensive comparison between LaneNet and SegNet. By evaluating their strengths and potential pitfalls in the domain of lane detection, we aim to provide insights that might guide future advancements in the field, determining which approach offers the most promise in real-world driving scenarios.
<br><br>
<!-- Approach -->
<h3>Approach</h3>
<ul>

<li> <b>Data Understanding and Preprocessing</b>
<br>
We will begin by examining the "Semantic Segmentation for Self Driving Cars" dataset, which has been sourced from the Lyft Udacity Challenge. This dataset provides us with a unique advantage of having sets of RGB images along with their corresponding semantic segments, allowing us to have clear ground truth for training and validation.
<br>
The dataset comprises five sets, each containing 1000 images and their associated labels. Our initial preprocessing steps will involve:
<ul class="sub">
<li>Checking for any missing values or corrupted images.</li>
<li>Normalizing the RGB values of the images for uniformity and to aid the training process.</li>
<li>Splitting the dataset into training, validation, and testing subsets, ensuring a good representation of each class in all the subsets.</li>
</ul>
</li>
<br>
<li> <b>Training with LaneNet</b>
<br>
Given our primary focus on LaneNet for lane detection, we will configure the LaneNet architecture to suit our dataset. 
<br>
Training will involve:
<ul class="sub">
<li>Feeding the normalized RGB images to LaneNet.</li>
<li>Using the corresponding semantic segments as ground truth to compute the loss and backpropagate the error.</li>
<li>Periodically validating the model's performance on the validation subset to monitor for overfitting and guide hyperparameter tuning.</li>
<li>Saving the best-performing model based on validation metrics for subsequent testing.</li>
</ul>
</li>
<br>
<li> <b>Testing in the CARLA Environment</b>
<br>
Post-training, the robustness and efficiency of our trained LaneNet model will be evaluated in the dynamic CARLA environment. By simulating real-world driving scenarios, CARLA provides a comprehensive testing ground where the model's predictions can be compared against the actual road and lane conditions.
</li>
<br>
<li> <b> Comparison with SegNet </b>
<br>
To gauge the efficacy of our chosen model, we will contrast LaneNet's performance with SegNet, another reputable model in the domain of semantic segmentation. 
<br>
The comparison will involve:
<ul class="sub">
<li>Training SegNet on the same training subset derived from our dataset.</li>
<li>Evaluating both models' performances based on metrics such as accuracy, F1-score, recall, and precision on the testing subset.</li>
<li>Analyzing how each model performs in the CARLA testing environment, understanding their strengths, weaknesses, and potential areas of improvement.</li>
</ul>
</li>
<br>
<li><b>Conclusion and Future Directions </b>
<br>
After rigorous training, testing, and comparison, we will consolidate our findings, highlighting the advantages and potential pitfalls of using LaneNet for lane detection in simulated environments. Our results will offer insights into the feasibility of deploying such models in real-world autonomous driving systems and provide a foundation for potential enhancements in future iterations.
</li>

</ul> 

<br><br>

<!-- Results -->
<h3>Expected Outcomes</h3>
The anticipated outcomes of this study encompass several key aspects. 
We expect to achieve a high level of accuracy in lane detection for both LaneNet and SegNet, with well-balanced precision and recall scores. 
Additionally, a comparative analysis is likely to reveal nuanced strengths, where LaneNet may excel in intricate lane markings, while SegNet might demonstrate superior performance in complex, diverse environments. We foresee robust adaptability across various lighting conditions and road types. 
Visual representations will provide clear illustrations of lane detection outputs, aiding in the assessment of model performance. Moreover, insights into potential areas for improvement, such as hyperparameter fine-tuning and enhanced data augmentation, are expected. 
<br><br>


<!-- References -->
<h3>References</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1807.01726">[1] LaneNet: Real-Time Lane Detection Networks for Autonomous Driving</a></li>
  <li><a href="https://arxiv.org/abs/1511.00561">[2] SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</a>
  <li><a href="https://carla.org/">[3] CARLA: Open-source simulator for autonomous driving research</a>
  <li><a href="https://www.kaggle.com/datasets/kumaresanmanickavelu/lyft-udacity-challenge?select=dataB">[4] Dataset Source</a>

  </ul>
<br><br>


  <hr>
  <footer> 
  <p>Â© Krishna Kanth Vuppala Narasimha, Beenaa Motiram Salian, Sriamoghavarsha Beerangi Srinivasa</p>
  </footer>
</div>
</div>

<br><br>

</body></html>