<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Course Project
  | ECE, Virginia Tech | Fall 2021: ECE 4554/5554</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {  
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
table, th, td, tr {
         border: 1px solid black;
      }
tr:hover {background-color: #D6EEEE;}

</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Lane Detection System For ADAS</h1> 
<span style="font-size: 16px; line-height: 1.5em;"><strong>Krishna Kanth Vuppala Narasimha, Beenaa Motiram Salian, Sriamoghavarsha Beerangi Srinivasa</strong></span><br>
<span style="font-size: 16px; line-height: 1.5em;">Fall 2023 ECE 4554/5554 Computer Vision: Course Project</span><br>
<span style="font-size: 16px; line-height: 1.5em;">Virginia Tech</span>
<hr>


<!-- Goal -->
<h3>Abstract</h3>
<p align="justify"; style = "font-size:16px";>
  &nbsp;&nbsp;&nbsp;&nbsp; With the increasing demand for safer and more efficient autonomous driving, our project tackles the crucial task of enhancing lane detection systems.
Accurate lane detection is pivotal for safe navigation and has far-reaching implications for the future of self-driving vehicles.
We adopted a comprehensive approach by implementing and comparing two advanced deep learning models, U-Net (LaneNet) and SegNet,
both trained on a diverse dataset of synthetically generated highway images. Our evaluation, conducted on a video generated by CARLA Simulator,
provides insights into the models' performance in real-world driving scenarios.
</p>
<br><br>

<!-- figure -->
<!-- Main Illustrative Figure --> 
<h3>Teaser figure</h3>
<br><br>
<div style="text-align: center;">
<img style="height: 150px;" alt="Teaser Image" src="teaser_img.png">
</div>
<br><br>
<div style="text-align: center;">
<img style="height: 200px;" alt="Teaser Image" src="https://thegradient.pub/content/images/2018/05/semseg.gif">
</div>
<br>
<br><br>

<!-- Introduction -->
<h3>Introduction</h3>
<p align="justify"; style = "font-size:16px">
&nbsp;&nbsp;&nbsp;&nbsp; In the rapidly evolving landscape of autonomous driving and advanced driving assistance systems, the accuracy of lane detection is crucial.
Ensuring reliable lane identification is vital for vehicle safety, supporting essential functionalities like lane-keeping and path planning, particularly in complex urban environments. 
Traditional computer vision methods, while foundational, often struggle with real-world variables such as inconsistent lane markings, variable lighting, and adverse weather conditions.
</p>

<p align="justify"; style = "font-size:16px">
&nbsp;&nbsp;&nbsp;&nbsp; This project explores the capabilities of deep learning in lane detection by harnessing U-Net and SegNet, two distinct models renowned for their segmentation prowess. 
U-Net, known for its precise segmentation abilities, is juxtaposed against SegNet, which excels in efficient semantic segmentation derived from the VGG16 architecture. 
Utilizing a synthetic dataset generated by the CARLA simulator, an open-source platform for autonomous driving research, this study trains and evaluates these models on a specially curated evaluation set. 
This comparative analysis aims to discern their performance and generalization capabilities in simulated real-world scenarios, contributing to the advancement of lane detection technologies in the realm of autonomous vehicles.
</p>

<br><br>
<!-- Approach -->
<h3>Approach</h3>
<ul>

<li style = "font-size:16px"> <b>Dataset Generation using CARLA </b>
<br>
<p align="justify"; style = "font-size:16px">
&nbsp;&nbsp;&nbsp;&nbsp;The first critical step in our approach involved the generation of a comprehensive dataset using the CARLA simulator. To create a dataset that is both extensive and diverse, 
we focused on seven distinct towns available within CARLA, each offering unique urban layouts and environmental conditions. From each town, we generated a total of 1500 images, 
accompanied by their respective segmentation masks, amounting to a primary dataset of 10500 images.
</p>

<p align="justify"; style = "font-size:16px">
&nbsp;&nbsp;&nbsp;&nbsp; This dataset was designed to capture a wide spectrum of driving scenarios, including variations in weather, lighting, and traffic conditions. 
The segmentation masks were meticulously crafted to accurately represent lane markings, ensuring high-quality ground truth data for model training.
</p>

<p align="justify"; style = "font-size:16px">
&nbsp;&nbsp;&nbsp;&nbsp; For the evaluation phase, we extended our dataset generation to include a eigth town, not previously seen by the models during training. 
This new town contributed an additional 1500 images with corresponding masks, specifically reserved for testing the models' performance and generalization capabilities.
</p>


<br>
<li style = "font-size:16px"> <b>Model Selection</b>

<p align="justify"; style = "font-size:16px">
&nbsp;&nbsp;&nbsp;&nbsp; For this project, we focused on two advanced deep learning models, each with distinct architectural strengths suited for the task of lane detection:
</p>

<ol class="sub">
  <li><b>U-Net (LaneNet)</b></li>
  <p align="justify"; style = "font-size:16px">
    &nbsp;&nbsp;&nbsp;&nbsp; In our quest to advance lane detection technology, we chose to delve into the capabilities of LaneNet, particularly focusing on the U-Net architecture, 
    which forms a significant part of LaneNet's design. LaneNet is known for its dual approach to lane detection, combining both semantic segmentation and instance segmentation. 
    However, for our project's specific focus on semantic segmentation - the task of classifying each pixel into a lane or non-lane category - we selected U-Net,
    a core component of LaneNet's architecture responsible for its binary segmentation capability.
  </p>
  <p align="justify"; style = "font-size:16px">
    The architecture of U-Net consists of the following features:
  </p>
  <ul class="sub">
    <li>Symmetrical Design: U-Net's architecture is characterized by its symmetrical design, consisting of two main parts: a contracting (downsampling) path and an expansive (upsampling) path, connected by a bottleneck.</li>
    <li>Contracting Path: The contracting path follows the typical architecture of a convolutional network. It comprises repeated application of two 3x3 convolutions (each followed by a rectified linear unit or ReLU), 
      and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step, U-Net doubles the number of feature channels.</li>
    <li>Bottleneck: This is the transition layer between the contracting and expansive paths. It usually involves two 3x3 convolutions with ReLU and sometimes includes dropout for regularization.</li>
    <li>Expansive Path: Each step in the expansive path consists of an up-sampling of the feature map followed by a 2x2 convolution that halves the number of feature channels, a concatenation with the corresponding cropped feature map from the contracting path, 
      and two 3x3 convolutions, each followed by ReLU. The expansive path recovers the spatial dimensions lost during downsampling.</li>
    <li>Skip Connections: A key feature of U-Net is the skip connections that link the layers of the contracting path to the corresponding layers in the expansive path. These connections help recover the spatial context lost in the downsampling process,
      which is critical for accurate pixel-wise classification.</li>
    <li>Output Layer: The final layer of the network is a 1x1 convolution that maps each feature vector to the desired number of classes.</li>
  </ul>

  <br>
  <li><b>SegNet</b></li>
  <p align="justify"; style = "font-size:16px">
    &nbsp;&nbsp;&nbsp;&nbsp; After exploring the U-Net architecture for its precise segmentation capabilities, we shift our focus to another pivotal model in our study - SegNet. The selection of SegNet is grounded in its architectural innovations and effectiveness in 
    semantic segmentation tasks, particularly for applications in autonomous driving like lane detection.
  </p>
  <p align="justify"; style = "font-size:16px">
    The architecture of SegNet consists of the following features:
  </p>
  <ul class="sub">
    <li>Basic Structure: SegNet's architecture is a deep convolutional encoder-decoder structure that draws its foundational design from the VGG16 model. It consists of a series of encoder layers followed by a corresponding set of decoder layers and a 
      final pixel-wise classification layer.</li>
    <li>Encoder: The encoder in SegNet is essentially similar to the VGG16 network, comprising 13 convolutional layers. These layers are grouped into five sets, each followed by a max-pooling operation. 
      The purpose of the encoder is to capture high-level contextual information from the input images. The key aspect here is that the max-pooling indices (the positions of the maximum feature value in each pooling window) are stored for each layer.</li>
    <li>Decoder: The decoder part of SegNet uses the stored max-pooling indices from the corresponding encoder to perform non-linear upsampling of the feature maps. This unique approach helps in better reconstruction of the input image resolution in the segmented output. 
      The decoder also consists of 13 convolutional layers grouped similarly to the encoder, but in reverse order. Each set of decoder layers upsamples its input feature map and then applies convolution to generate denser feature maps.</li>
    <li>Output Layer: The final layer of the network is a 1x1 convolution that maps each feature vector to the desired number of classes.</li>
    </ul>
</ol>
</li>


<br><br>



<h3>Experimentation</h3>
<br>
<ol class="sub">
  <li style = "font-size:16px"><b>Dataset Curation</b></li>
  <p align="justify"; style = "font-size:16px">
  &nbsp;&nbsp;&nbsp;&nbsp; The dataset, vital for training and evaluating our models, was curated using images generated from the CARLA simulator. The initial step in dataset curation involved splitting the data to create distinct sets for training, validation, and testing. 
  We ensured a balanced representation of various driving scenarios across these sets.
  </p>
  <ul class="sub"; style = "font-size:16px">
    <li>Training Set: Comprised 60% of the total images, serving as the primary source for model training.</li>
    <li>Validation Set: Accounted for 20% of the images, used to fine-tune model parameters and prevent overfitting.</li>
    <li>Testing Set: The remaining 20% formed the testing set, reserved for evaluating the model performance post-training.</li>
  </ul>


<br>

<p align="justify"; style = "font-size:14px"> Generated Dataset - 7 towns: 1500 Images </p>

<table>
  <caption style="font-size: 14px;"> Dataset </caption>
<tr>
  <th style="padding:10px" >Set</th>
  <th style="padding:10px" >Train</th>
  <th style="padding:10px" >Val</th>
  <th style="padding:10px" >Test</th>
  <th style="padding:10px" >Total</th>
</tr>
<tr>
  <th style="padding:10px" >Images</th>
  <th style="padding:10px" >6300</th>
  <th style="padding:10px" >2100</th>
  <th style="padding:10px" >2100</th>
  <th style="padding:10px" >10500</th>
</tr>
<tr>
  <th style="padding:10px" >Masks</th>
  <th style="padding:10px" >6300</th>
  <th style="padding:10px" >2100</th>
  <th style="padding:10px" >2100</th>
  <th style="padding:10px" >10500</th>
</tr>
<tr>
  <th style="padding:10px" >Proportion</th>
  <th style="padding:10px" >60%</th>
  <th style="padding:10px" >20%</th>
  <th style="padding:10px" >20%</th>
  <th style="padding:10px" >100%</th>
</tr>
</table>

<br>
<p align="justify"; style = "font-size:14px"> Evaluation Dataset - 1 towns: 1500 Images (Exclusive to model train/test data) </p>

<br>

  <li style = "font-size:16px"><b>Preprocessing</b></li>
  <p align="justify"; style = "font-size:16px">
    Preprocessing is a crucial step to ensure that the data fed into our models is clean and standardized. This phase involved:
  </p>
  <ul class="sub"; style = "font-size:16px">
    <li>Image Normalization: Scaling the RGB values of the images to a [0, 1] range to facilitate smoother training.</li>
    <li>Mask Processing: Converting the segmentation masks to a format suitable for semantic segmentation. 
      This involved resizing the masks to match the image dimensions and extracting relevant channels that represent lane markings.</li>
    <li>Resizing: Downscaling images and masks to reduce dimensionality of data to make it easier for deep learning networks to learn.</li>
  </ul>

  <br>

  <li style = "font-size:16px"><b>Model training</b></li>
  <p align="justify"; style = "font-size:16px">
  &nbsp;&nbsp;&nbsp;&nbsp; The training of U-Net and SegNet models was a crucial phase of our experiment. 
  Each model underwent a rigorous training process using the TensorFlow and Keras frameworks, adhering to specific architectural guidelines and hyperparameters. 
  Our approach to training each model was methodical, ensuring that both models were optimized for the best possible performance on lane detection tasks.
  </p>

  <h5>U-Net Training Process </h5>
  <ul class="sub";style = "font-size:16px" >
    <li style = "font-size:16px">Architecture Setup: The U-Net model was constructed with a sequence of encoder and decoder blocks, each consisting of convolutional layers with ReLU activation 
      and batch normalization. The encoder blocks used max pooling for downsampling, while the decoder blocks utilized transposed convolutions for upsampling.</li>
    <li style = "font-size:16px">Compilation: The model was compiled with the Adam optimizer and binary cross-entropy loss function, suitable for the binary classification nature of our lane detection task. 
      Performance metrics included accuracy, Dice coefficient, and Intersection over Union (IoU).</li>
    <li style = "font-size:16px">Callbacks: To enhance training efficiency, callbacks such as Early Stopping and ReduceLROnPlateau were used. Early Stopping prevented overfitting by halting the training if the validation loss 
      did not improve for five epochs, while ReduceLROnPlateau reduced the learning rate when the validation loss plateaued, ensuring finer adjustments to the model weights.    </li>
    <li style = "font-size:16px">Training Execution: The model was trained for 20 epochs with a batch size of 32, using the prepared training and validation datasets. Data was fed into the model in batches for efficient memory utilization.</li>
  </ul>
  
  <h5>SegNet Training Process </h5>
  <ul class="sub"; style = "font-size:16px">
    <li style = "font-size:16px">Architecture Construction: SegNet's architecture, built on an encoder-decoder structure, involved custom layers for max pooling with argmax indices and corresponding unpooling in the decoder. This unique setup was crucial for capturing and utilizing spatial information efficiently. </li>
    <li style = "font-size:16px">Compilation Details: Similar to U-Net, SegNet was compiled using the Adam optimizer and binary cross-entropy loss. Accuracy, Dice coefficient, and IoU were also used as metrics.</li>  
    <li style = "font-size:16px">Callbacks Implementation: The same set of callbacks used for U-Net, including Early Stopping and ReduceLROnPlateau, were employed for SegNet to ensure optimal training performance.</li>
    <li style = "font-size:16px">Training Process: SegNet's training was conducted for 20 epochs with a batch size of 32, considering the model's architectural complexity and memory requirements.</li>
  </ul>

  <p align="justify"; style = "font-size:16px">
  &nbsp;&nbsp;&nbsp;&nbsp; Both models' training was monitored for performance metrics on the validation set, ensuring that the models were learning to generalize beyond the training data. The ModelCheckpoint callback was employed to save the best version of each model based on the validation IoU,
  ensuring that we retained the model iteration with the highest segmentation accuracy.
  </p>

  <br>
  <li style = "font-size:16px"><b>Evaluation</b></li>
  <p align="justify"; style = "font-size:16px">
    &nbsp;&nbsp;&nbsp;&nbsp; The final stage of our experiment was the evaluation of the trained models. This involved:
  </p>
  <ul class="sub"; style = "font-size:16px">
    <li style = "font-size:16px">Testing on Unseen Data: Both models were tested on the reserved testing set and an additional 'surprise' set obtained from a new CARLA environment, to assess their generalization capabilities.</li>
    <li style = "font-size:16px">Performance Metrics: We evaluated the models based on loss, accuracy, Dice coefficient, and IoU. These metrics provided insights into the models' effectiveness in segmenting lane markings accurately.</li>
    <li style = "font-size:16px">Visual Inspection: Besides quantitative metrics, we also conducted a visual inspection of the predicted segmentation masks against the ground truth. This qualitative evaluation helped in understanding the practical applicability of the models in real-world scenarios.</li>
  
  </ul>
</ol>


<br>
<br>
<h3>Results</h3>
<br>
<ol class="sub">
  <li style = "font-size:16px"><b>Quantitative Results</b></li>
  <ul class="sub">
    <li style = "font-size:16px">U-Net</li>
    <br>
    <div style="display: flex;
    flex-direction: row;
    align-content: center;
    justify-content: center;" >

    
    <table>
      <caption style="font-size: 14px;"> Model 1: U-Net (20 epochs - early stopping and checkpointing) </caption>
    <tr>
      <th style="padding:10px" >DataSet</th>
      <th style="padding:10px" >Set</th>
      <th style="padding:10px" >dice</th>
      <th style="padding:10px" >iou</th>
    </tr>
    <tr>
      <td style="padding:10px"; rowspan="3" >Original Dataset</td>
      <th style="padding:10px" >Train</th>
      <th style="padding:10px" >84.51</th>
      <th style="padding:10px" >93.40</th>
    </tr>
    <tr>
      <th style="padding:10px" >Val</th>
      <th style="padding:10px" >84.52</th>
      <th style="padding:10px" >93.33</th>
    </tr>
    <tr>
      <th style="padding:10px" >Test</th>
      <th style="padding:10px" >84.63</th>
      <th style="padding:10px" >93.39</th>
    </tr>
    <tr>
      <th style="padding:10px" >Evaluation Dataset</th>
      <th style="padding:10px" >full</th>
      <th style="padding:10px" >66.10</th>
      <th style="padding:10px" >88.77</th>
    </tr>
    </table>
    </div>
    <br><br>
    <li style = "font-size:16px">SegNet</li>
    <br>
    <div style="display: flex;
    flex-direction: row;
    align-content: center;
    justify-content: center;" >
    <table>
      <caption style="font-size: 14px;"> Model 2: SegNet (20 epochs - early stopping and checkpointing) </caption>
    <tr>
      <th style="padding:10px" >DataSet</th>
      <th style="padding:10px" >Set</th>
      <th style="padding:10px" >dice</th>
      <th style="padding:10px" >iou</th>
    </tr>
    <tr>
      <td style="padding:10px"; rowspan="3" >Original Dataset</td>
      <th style="padding:10px" >Train</th>
      <th style="padding:10px" >73.75</th>
      <th style="padding:10px" >89.17</th>
    </tr>
    <tr>
      <th style="padding:10px" >Val</th>
      <th style="padding:10px" >73.93</th>
      <th style="padding:10px" >89.26</th>
    </tr>
    <tr>
      <th style="padding:10px" >Test</th>
      <th style="padding:10px" >73.55</th>
      <th style="padding:10px" >89.28</th>
    </tr>
    <tr>
      <th style="padding:10px" >Evaluation Dataset</th>
      <th style="padding:10px" >full</th>
      <th style="padding:10px" >43.57</th>
      <th style="padding:10px" >79.18</th>
    </tr>
    </table>
    </div>
    <br>
  </ul>
  <br>
  <p align="justify"; style = "font-size:16px">
  <b>Architecture efficiency:</b> U-Net's architecture is more efficient in learning relevant features from the given dataset due to its symmetric structure and the use of skip connections. 
  These connections help combine low-level feature details with high-level context, which is critical in semantic segmentation for capturing the precise shape and location of lanes. 
  U-Net achieved Dice coefficients of 84.51%, 84.52%, and 84.63% on the train, validation, and test sets respectively, and an IoU of 93.4%, 93.33%, and 93.39% on the same sets, 
  indicating its proficiency in capturing lane details. In contrast, SegNet's decoder does not benefit from skip connections, potentially leading to less accurate reconstruction of the 
  segmentation maps from the compressed feature representation through relaying pooling indices only. This is reflected in its lower Dice scores of 73.75%, 73.93%, and 73.55% and IoU scores of
  89.17%, 89.26%, and 89.28% on the train, validation, and test sets respectively.
  </p>

  <p align="justify"; style = "font-size:16px">
  <b>Generalization Ability: </b>The drop in performance on the full evaluation dataset suggests that while both models generalize beyond the training data, U-Net does so more effectively,
  with a Dice score of 66.1% and an IoU of 88.77%, compared to SegNet's considerable decline. U-Net's ability to preserve and utilize fine-grained spatial information is crucial for 
  generalizing to images of unseen towns or varied environmental conditions produced by the CARLA simulator.
  </p>

  <br>
  <li style = "font-size:16px"><b>Qualitative Results</b></li>
  <ul class="sub">
    <li style = "font-size:16px">U-Net</li>
    <div style="text-align: left;">
    <img style="height: 450px;" alt="U-Net Results" src="Unet_op.jpeg">
    </div>
    <iframe width="400" height="300" src="https://www.youtube.com/embed/xzOGRTme_Vo" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"></iframe><br><br>

    <li style = "font-size:16px">SegNet</li>
    <div style="text-align: left;">
    <img style="height: 450px;" alt="SegNet Results" src="Segnet_op.jpeg">
    </div>

  </ul>
<br>
<br>
  <p align="justify"; style = "font-size:16px">
    Upon observing the visual predictions of U-Net and SegNet in comparison to the true masks, we can see that U-Net is capable of capturing fine grained details of the lane marking much better 
    than SegNet. This can again be explained by the architecture differences between the two networks that help U-Net in creating a more precise and accurate segmentation map.
  </p>
</ol>

<br><br>
<h3>Conclusion</h3>
<p align="justify"; style = "font-size:16px">
  &nbsp;&nbsp;&nbsp;&nbsp; The exploration into lane detection using U-Net and SegNet conducted within the simulated environments of CARLA has yielded significant insights into the efficacy of these deep learning models. 
  Lane detection is a critical component in the advancement of autonomous driving technologies, where the accurate identification and segmentation of lane markings are crucial for navigation and safety. 
  In such a context, the choice of the right model for semantic segmentation becomes pivotal.
</p>

<p align="justify"; style = "font-size:16px">
  &nbsp;&nbsp;&nbsp;&nbsp; CARLA, as a versatile and sophisticated simulator, played a crucial role in this study by providing a diverse and realistic dataset for training and evaluating our models. 
  The ability to simulate various driving conditions, from urban landscapes to different weather scenarios, allowed us to test the robustness and adaptability of U-Net and SegNet in environments that closely mimic real-world conditions.
   This comprehensive approach to dataset generation ensured that our findings would be relevant and applicable to actual autonomous driving systems.
</p>

<p align="justify"; style = "font-size:16px">
  &nbsp;&nbsp;&nbsp;&nbsp; In our comparative analysis, U-Net distinguished itself as the more capable model for high-fidelity semantic segmentation in the context of lane detection. Its architecture, featuring a symmetric design with skip connections, 
  excelled in maintaining essential feature details throughout the network. This capability translated into superior performance in both quantitative and qualitative assessments, particularly in preserving the intricacy and accuracy required for lane detection. 
  On the other hand, SegNet, while demonstrating commendable segmentation capabilities, fell short in aspects of feature propagation and generalization. These findings highlight U-Net's potential as a more suitable and reliable choice for lane detection tasks in autonomous driving, 
  underscoring the importance of architectural considerations in the development of deep learning models for such critical applications.
</p>

<br>
<br>



<!-- References -->
<h3>References</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1807.01726">[1] LaneNet: Real-Time Lane Detection Networks for Autonomous Driving</a></li>
  <li><a href="https://arxiv.org/abs/1511.00561">[2] SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</a>
  <li><a href="https://carla.org/">[3] CARLA: Open-source simulator for autonomous driving research</a>
  </ul>
<br><br>

  <hr>
  <footer> 
  <p>Â© Krishna Kanth Vuppala Narasimha, Beenaa Motiram Salian, Sriamoghavarsha Beerangi Srinivasa</p>
  </footer>
</div>
</div>

<br><br>

</body></html>